# Spike-and-Slab Stick-breaking Process for Finding the Optimal Number of Clusters

Cheng Zeng and Leo L. Duan

***abstract***: In non-parametric mixture models, the stick-breaking process is an appealing construction that allows the number of mixture components to grow indefinitely. However, the posterior tends to over-estimate the number of components, due to the small but non-trivial weights appearing spuriously. In this article, we propose a simple change to the stick-breaking process: at each break the proportion takes a spike-and-slab form, with the slab as a Beta random variable and the spike as a close-to-zero constant. This allows us to shrink the small mixture weights much closer to zero, while avoiding the singularity issue caused by zero probability. We show in theory this leads to a more parsimonious clustering model, and a constistent estimator for the number of mixture components. The computation enjoys closed-form in efficient collapsed Gibbs sampling. The experiments show substantial reduction in the number of components.

Key Words: Sparse Simplex, Infinte Mixture, Posterior Consistency.

### 1. Introduction:

Stick-breaking

Pitman-Yor

Finite mixture model with prior on the number of components


### 2. Spike-and-Slab Stick-Breaking Process

We consider the following generative process,

$$
\begin{aligned}
& \theta_i \sim\sum_{k=1}^{\infty} w_k \delta_{\theta^*_k}(.), \quad
\theta^*_k \sim \mathcal{G}_0, \qquad y_i \sim \mathcal{F}(\theta_i) \\
& w_1 = v_1, \qquad w_k = v_k\prod_{l=1}^{k-1} (1-v_l) \text{ for } k>1\\
& v_k = p \delta_{\beta_k}(.)   + (1-p) \delta_\epsilon(.), \\
& \beta_k\sim  \text{Beta}(1,\alpha),
\end{aligned}
$$
where $\mathcal F$ denotes the component distribution for $y_i$, parameterized by $\theta_i$; and $\mathcal G_0$ is the base measure. For simplicity, we choose $\beta_k \sim \text{Beta}(1,\alpha)$ in this article; and if $p$ were equal to $1$, this model would coincide with the Dirichlet process with concentration parameter $\alpha$.

We refer to $v_k$ as the break proportion from now on. With $0<p<1$, this takes a spike-and-slab form, with the `spike' occuring at value $\epsilon$. We set $\epsilon\approx 0$, but strictly positive. This avoids the singularity issue due to zero probability.

Given the latent assignment variable $c_i=k$, the joint likelihood of $(y_i,c_i)$'s can be written as

$$
\begin{aligned}
& L( \{y_i,c_i\}_{i=1}^n ; \{\theta^*_k, v_k\}_{k=1}^{\infty} )  =  \prod_{i=1}^n f(y_i ; \theta^*_{c_i})\prod_{k=1}^{\infty}
 v_k^{n_k} (1- v_k) ^{m_{k}},\\
 & v_k = p \delta_{\beta_k}(.)   + (1-p) \delta_\epsilon(.),
\end{aligned}
$$
where $n_k=\sum_{i=1}^n 1(c_i=k)$ and $m_{k}=\sum_{i=1}^n 1(c_i>k)$. The posterior of $\beta_k$ is $\text{Beta}(n_k+1, m_k + \alpha)$, by marginalizing it out we have

$$
\begin{aligned}
  \text{pr}(v_k = \epsilon \mid .)  =
\frac{ (1-p)\epsilon^{n_k} (1- \epsilon) ^{m_{k}}}{p \alpha \text{B}(n_k+1, m_k + \alpha) 
 + (1-p)\epsilon^{n_k} (1- \epsilon) ^{m_{k}}} 
\end{aligned}
$$
When $n_k$ is small, this probability is large. As a special case, when $n_k=m_k=0$, we have $\text{pr}(v_k = \epsilon \mid .)= 1-p$; since $w_k \le v_k$, using a small $p$, this provides a strong shrinkage to reduce the weights of empty components to be smaller than $\epsilon$. Figure 1 compares how the spike-and-slab formation changes the posterior distribution of $v_k$ at small $n_k$.




Figure 1: The posterior probability of having a small break proportion $v_k \leq 0.001$:  for those with small $n_k$, using the spike-and-slab creates a strong shrikage to have the mixture weight $w_k \leq v_k\approx 0$.

### 3. Collapsed Gibbs Sampler

### 4. Properties

Exchangeable Partition Probability Function

Let $g_k = n_k+m_k$, and $M(c)= \sup_k \{k: c_k>0\}$. 

$$
\begin{aligned}
 L & ( c_1, c_2,\ldots, c_n)  =  \prod_{k=1}^{\infty} 
\bigg\{ p \alpha \text{B}(n_k+1,  g_{k+1} + \alpha) 
 + (1-p)\epsilon^{n_k} (1- \epsilon) ^{g_{k+1}}
 \bigg \}  \\
 \stackrel{(a)}{=} & \prod_{k=1}^{M(c)} 
\bigg\{ p \alpha \text{B}(n_k+1, g_{k+1} + \alpha) 
 + (1-p)\epsilon^{n_k} (1- \epsilon) ^{g_{k+1}}
 \bigg \},
\end{aligned}
$$
where $(a)$ is using $\alpha B(1,\alpha)=1$ when $n_k=m_k=0$. For simplicity, let $Q_k = p \alpha \text{B}(n_k+1, g_{k+1} + \alpha)  + (1-p)\epsilon^{n_k} (1- \epsilon) ^{g_{k+1}}.$

Let $k_1<\ldots<k_t$ denote the unique values of $(c_1,\ldots,c_n)$, and set $k_0=0$.

For $k_{i-1}<k<k_{i}$, we have $n_k=0$ and $g_{k+1} = g_{k_i}$. This leads to

$$
Q_k = p  \frac{\alpha}{g_{k_i} + \alpha} + (1-p) (1- \epsilon) ^{g_{k_i}}.
$$

For $k=k_{i}$, we have $n_k=n_{k_i}$ and $g_{k+1} = g_{k_{i+1}}$. This leads to

$$
Q_k = p \alpha \frac{ \Gamma(n_{k_i}+1) \Gamma(g_{k_{i+1}} + \alpha)}{\Gamma(n_{k_i}+1+g_{k_{i+1}} + \alpha)}
 + (1-p)\epsilon^{n_{k_i}} (1- \epsilon) ^{g_{k_{i+1}}}.
$$

Combining these two we have,

$$
\begin{aligned}
 L  ( c_1, c_2,\ldots, c_n) =
\prod_{i=1}^{t}  \bigg\{ p  \frac{\alpha}{g_{k_i} + \alpha} + (1-p) (1- \epsilon) ^{g_{k_i}} \bigg\}^{(k_i-k_{i-1})-1} \\
\times
\bigg\{
  p \alpha \frac{ \Gamma(n_{k_i}+1) \Gamma(g_{k_{i+1}} + \alpha)}{\Gamma(n_{k_i}+1+g_{k_{i+1}} + \alpha)}
 + (1-p)\epsilon^{n_{k_i}} (1- \epsilon) ^{g_{k_{i+1}}}
 \bigg \}.
 \end{aligned}
$$

Let $d_i = k_i-k_{i-1}-1$, hence $d_i=0,1,2,\ldots$. Letting $n^*_i$ denote the value of $n_{k_i}$ and $g^*_i$ the value of $g_{k_i}$, summing out all possible values of $d_i$,

$$
\begin{aligned}
 & \sum_{\text{each } d_i\in \mathbb N} L  ( c_1, c_2,\ldots, c_n) 1(n_{k_i}=n^*_i \text{ for } i=1,\ldots,t)\\
 & =
 \prod_{i=1}^t \bigg [ \bigg\{
  p \alpha \frac{ \Gamma(n^*_{i}+1) \Gamma(g^*_{{i+1}} + \alpha)}{\Gamma(n^*_{i}+1+g^*_{{i+1}} + \alpha)}
 + (1-p)\epsilon^{n^*_{i}} (1- \epsilon) ^{g^*_{{i+1}}}
 \bigg \}\\
 &\times \sum_{{d_i=0}}^{\infty} \bigg\{ p  \frac{\alpha}{g^*_{i} + \alpha} + (1-p) (1- \epsilon) ^{g^*_{i}} \bigg\}^{d_i} \bigg ]\\
  & \stackrel{(a)}{=}
 \prod_{i=1}^t\bigg [ \bigg\{
  p \alpha \frac{ \Gamma(n^*_{i}+1) \Gamma(g^*_{{i+1}} + \alpha)}{\Gamma(n^*_{i}+1+g^*_{{i+1}} + \alpha)}
 + (1-p)\epsilon^{n^*_{i}} (1- \epsilon) ^{g^*_{{i+1}}}
 \bigg \}\\
 &\times \bigg\{ 1-p  \frac{\alpha}{g^*_{i} + \alpha} - (1-p) (1- \epsilon) ^{g^*_{i}} \bigg\}^{-1} \bigg ],
 \end{aligned}
$$
where $(a)$ is based on $\sum_{d=0}^{\infty} x^d = (1-x)^{-1}$ with $|x|<1$.


Predictive Distribution

Posterior Consistency

### 5. Data Experiments

